{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Lab - 345.2.3 - PYSQL\n",
    "## Create DataFrame From the Data Sources(CSV and JSON)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab Overview\n",
    "\n",
    "\n",
    "In this lab, we will first create a SparkSession. Then, we will create a new data frame by importing CSV files and JSON files.\n",
    "\n",
    "\n",
    "Example 1 - Creating the DataFrame from CSV File\n",
    "Click here to Download the dataset about cars:\n",
    "\n",
    "Open the Jupyter notebook.\n",
    "Create a DataFrame by applying createDataFrame on RDD with the help of SparkSession as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|quantity|   city|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|AMC Ambassador Br...|13.0|        8|       360.0|       175|  3821|        11.0|   73|    US|      25|NewYork|\n",
      "|  AMC Ambassador DPL|15.0|        8|       390.0|       190|  3850|         8.5|   70|    US|       2|     NJ|\n",
      "|  AMC Ambassador SST|17.0|        8|       304.0|       150|  3672|        11.5|   72|    US|       4| DALLAS|\n",
      "|         AMC Concord|19.4|        6|       232.0|        90|  3210|        17.2|   78|    US|      52|  TEXAS|\n",
      "|         AMC Concord|24.3|        4|       151.0|        90|  3003|        20.1|   80|    US|      42|     OH|\n",
      "|     AMC Concord d/l|18.1|        6|       258.0|       120|  3410|        15.1|   78|    US|       4|NewYork|\n",
      "|      AMC Concord DL|23.0|        4|       151.0|         0|  3035|        20.5|   82|    US|      45|     NJ|\n",
      "|    AMC Concord DL 6|20.2|        6|       232.0|        90|  3265|        18.2|   79|    US|     328| DALLAS|\n",
      "|         AMC Gremlin|21.0|        6|       199.0|        90|  2648|        15.0|   70|    US|      68|  TEXAS|\n",
      "|         AMC Gremlin|19.0|        6|       232.0|       100|  2634|        13.0|   71|    US|      78|     OH|\n",
      "|         AMC Gremlin|18.0|        6|       232.0|       100|  2789|        15.0|   73|    US|     152|NewYork|\n",
      "|         AMC Gremlin|20.0|        6|       232.0|       100|  2914|        16.0|   75|    US|     214|     NJ|\n",
      "|          AMC Hornet|18.0|        6|       199.0|        97|  2774|        15.5|   70|    US|      60| DALLAS|\n",
      "|          AMC Hornet|18.0|        6|       232.0|       100|  2945|        16.0|   73|    US|     144|  TEXAS|\n",
      "|          AMC Hornet|19.0|        6|       232.0|       100|  2901|        16.0|   74|    US|     172|     OH|\n",
      "|          AMC Hornet|22.5|        6|       232.0|        90|  3085|        17.6|   76|    US|      28|NewYork|\n",
      "|AMC Hornet Sporta...|18.0|        6|       258.0|       110|  2962|        13.5|   71|    US|      90|     NJ|\n",
      "|         AMC Matador|18.0|        6|       232.0|       100|  3288|        15.5|   71|    US|      82| DALLAS|\n",
      "|         AMC Matador|14.0|        8|       304.0|       150|  3672|        11.5|   73|    US|     131|  TEXAS|\n",
      "|         AMC Matador|16.0|        6|       258.0|       110|  3632|        18.0|   74|    US|     179|NewYork|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession #Importing the Libraries\n",
    "# Creating Spark Session\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "# Reading /loading the Dataset from CSV file\n",
    "cardf = spark.read.load(\"cars.csv\", format=\"csv\", header = True,inferSchema = True)\n",
    "\n",
    "cardf.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a CSV file, simply specify the path to the load() function of the read module. The inferSchema and header  parameters are mandatory whenever reading CSV files; without them, Spark will cast every data type to string, and treat the header row as actual data:\n",
    "\n",
    "To see the types of columns in DataFrame, we can use the printSchema() method. Let’s apply printSchema() on cars.css files, which will print the schema in a tree format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Car: string (nullable = true)\n",
      " |-- MPG: double (nullable = true)\n",
      " |-- Cylinders: integer (nullable = true)\n",
      " |-- Displacement: double (nullable = true)\n",
      " |-- Horsepower: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      " |-- Acceleration: double (nullable = true)\n",
      " |-- Model: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- quantity: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Car',\n",
       " 'MPG',\n",
       " 'Cylinders',\n",
       " 'Displacement',\n",
       " 'Horsepower',\n",
       " 'Weight',\n",
       " 'Acceleration',\n",
       " 'Model',\n",
       " 'Origin',\n",
       " 'quantity',\n",
       " 'city']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let’s have a look at the column’s names.\n",
    "\n",
    "cardf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|quantity|   city|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|AMC Ambassador Br...|13.0|        8|       360.0|       175|  3821|        11.0|   73|    US|      25|NewYork|\n",
      "|  AMC Ambassador DPL|15.0|        8|       390.0|       190|  3850|         8.5|   70|    US|       2|     NJ|\n",
      "|  AMC Ambassador SST|17.0|        8|       304.0|       150|  3672|        11.5|   72|    US|       4| DALLAS|\n",
      "|         AMC Concord|19.4|        6|       232.0|        90|  3210|        17.2|   78|    US|      52|  TEXAS|\n",
      "|         AMC Concord|24.3|        4|       151.0|        90|  3003|        20.1|   80|    US|      42|     OH|\n",
      "|     AMC Concord d/l|18.1|        6|       258.0|       120|  3410|        15.1|   78|    US|       4|NewYork|\n",
      "|      AMC Concord DL|23.0|        4|       151.0|         0|  3035|        20.5|   82|    US|      45|     NJ|\n",
      "|    AMC Concord DL 6|20.2|        6|       232.0|        90|  3265|        18.2|   79|    US|     328| DALLAS|\n",
      "|         AMC Gremlin|21.0|        6|       199.0|        90|  2648|        15.0|   70|    US|      68|  TEXAS|\n",
      "|         AMC Gremlin|19.0|        6|       232.0|       100|  2634|        13.0|   71|    US|      78|     OH|\n",
      "|         AMC Gremlin|18.0|        6|       232.0|       100|  2789|        15.0|   73|    US|     152|NewYork|\n",
      "|         AMC Gremlin|20.0|        6|       232.0|       100|  2914|        16.0|   75|    US|     214|     NJ|\n",
      "|          AMC Hornet|18.0|        6|       199.0|        97|  2774|        15.5|   70|    US|      60| DALLAS|\n",
      "|          AMC Hornet|18.0|        6|       232.0|       100|  2945|        16.0|   73|    US|     144|  TEXAS|\n",
      "|          AMC Hornet|19.0|        6|       232.0|       100|  2901|        16.0|   74|    US|     172|     OH|\n",
      "|          AMC Hornet|22.5|        6|       232.0|        90|  3085|        17.6|   76|    US|      28|NewYork|\n",
      "|AMC Hornet Sporta...|18.0|        6|       258.0|       110|  2962|        13.5|   71|    US|      90|     NJ|\n",
      "|         AMC Matador|18.0|        6|       232.0|       100|  3288|        15.5|   71|    US|      82| DALLAS|\n",
      "|         AMC Matador|14.0|        8|       304.0|       150|  3672|        11.5|   73|    US|     131|  TEXAS|\n",
      "|         AMC Matador|16.0|        6|       258.0|       110|  3632|        18.0|   74|    US|     179|NewYork|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference: Here, we can see that the show() function has returned the top 20 rows of the dataset. Note that we have kept the header type as True so that Spark will treat the first row as header, and inferSchema is also set to True so that it returns the values with the real data type.\n",
    "\n",
    "We can use the head() operation to see the first N observation (say, 5 observations). Head operation in PySpark is similar to head operation in Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Car='AMC Ambassador Brougham', MPG=13.0, Cylinders=8, Displacement=360.0, Horsepower=175, Weight=3821, Acceleration=11.0, Model=73, Origin='US', quantity=25, city='NewYork'),\n",
       " Row(Car='AMC Ambassador DPL', MPG=15.0, Cylinders=8, Displacement=390.0, Horsepower=190, Weight=3850, Acceleration=8.5, Model=70, Origin='US', quantity=2, city='NJ'),\n",
       " Row(Car='AMC Ambassador SST', MPG=17.0, Cylinders=8, Displacement=304.0, Horsepower=150, Weight=3672, Acceleration=11.5, Model=72, Origin='US', quantity=4, city='DALLAS'),\n",
       " Row(Car='AMC Concord', MPG=19.4, Cylinders=6, Displacement=232.0, Horsepower=90, Weight=3210, Acceleration=17.2, Model=78, Origin='US', quantity=52, city='TEXAS'),\n",
       " Row(Car='AMC Concord', MPG=24.3, Cylinders=4, Displacement=151.0, Horsepower=90, Weight=3003, Acceleration=20.1, Model=80, Origin='US', quantity=42, city='OH')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardf.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the last rows means getting the last N rows from the given data frame. For this, we are using the tail() function and can get the last N rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Car='Volvo 264gl', MPG=17.0, Cylinders=6, Displacement=163.0, Horsepower=125, Weight=3140, Acceleration=13.6, Model=78, Origin='Europe', quantity=320, city='NewYork'),\n",
       " Row(Car='Volvo Diesel', MPG=30.7, Cylinders=6, Displacement=145.0, Horsepower=76, Weight=3160, Acceleration=19.6, Model=81, Origin='Europe', quantity=406, city='NJ')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardf.tail(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many columns do we have in cars.csv files along with their names?\n",
    "\n",
    "For getting the column's name, we can use columns on DataFrame; similar to what we do for getting the columns in Pandas DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cardf.columns)  # show number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Car',\n",
       " 'MPG',\n",
       " 'Cylinders',\n",
       " 'Displacement',\n",
       " 'Horsepower',\n",
       " 'Weight',\n",
       " 'Acceleration',\n",
       " 'Model',\n",
       " 'Origin',\n",
       " 'quantity',\n",
       " 'city']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardf.columns     # show name of the columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the summary statistics (mean, standard deviance, minimum, maximum, and count) of the numerical columns in a DataFrame.\n",
    "\n",
    "The describe() operation is used to calculate the summary statistics of numerical column(s) in DataFrame. \n",
    "\n",
    "If we do not specify the name of columns, it will calculate summary statistics for all numerical columns present in DataFrame.\n",
    "\n",
    "Let’s check to see what happens when we specify the name of a categorical /String column in the describe() operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|                 Car|\n",
      "+-------+--------------------+\n",
      "|  count|                 406|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|AMC Ambassador Br...|\n",
      "|    max|        Volvo Diesel|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.describe('Car').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the orderBy() operation on DataFrame to get sorted output based on some columns. \n",
    "\n",
    "The orderBy operation takes two arguments:\n",
    "\n",
    "List of columns.\n",
    "Ascending = True or False for getting the results in ascending or descending order (list in case of more than two columns).\n",
    "\n",
    "Let’s sort the cars' DataFrame based on Acceleration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|quantity|   city|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "|         Peugeot 504|27.2|        4|       141.0|        71|  3190|        24.8|   79|Europe|     344|NewYork|\n",
      "|   Volkswagen Pickup|44.0|        4|        97.0|        52|  2130|        24.6|   82|Europe|      96|NewYork|\n",
      "|Volkswagen Dasher...|43.4|        4|        90.0|        48|  2335|        23.7|   80|Europe|     371| DALLAS|\n",
      "|   Volkswagen Type 3|23.0|        4|        97.0|        54|  2254|        23.5|   72|Europe|     104|NewYork|\n",
      "|  Chevrolet Chevette|29.0|        4|        85.0|        52|  2035|        22.2|   76|    US|     240|  TEXAS|\n",
      "|Oldsmobile Cutlas...|23.9|        8|       260.0|        90|  3420|        22.2|   79|    US|     345|NewYork|\n",
      "|     Chevrolet Woody|24.5|        4|        98.0|        60|  2164|        22.1|   76|    US|     241|     OH|\n",
      "|         Peugeot 504|19.0|        4|       120.0|        88|  3270|        21.9|   76|Europe|     254|     OH|\n",
      "|  Mercedes-Benz 240d|30.0|        4|       146.0|        67|  3250|        21.8|   80|Europe|     373| DALLAS|\n",
      "|Volkswagen Rabbit...|44.3|        4|        90.0|        48|  2085|        21.7|   80|Europe|     370|     NJ|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+--------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cardf.orderBy(cardf.Acceleration.desc()).show(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2  - Creating the DataFrame from JSON File\n",
    "Download Dataset, and click the below links for JSON files:\n",
    "Zipcode1.json\n",
    "Zipcode2.json\n",
    "Zipcode.json\n",
    "multiline-zipcode.json\n",
    " \n",
    "Read JSON file into dataframe into dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- EstimatedPopulation: long (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- RecordNumber: long (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- TaxReturnsFiled: long (nullable = true)\n",
      " |-- TotalWages: long (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Xaxis: double (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- Zipcode: long (nullable = true)\n",
      "\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|EstimatedPopulation|  Lat|            Location|        LocationText|  LocationType|   Long|        Notes|RecordNumber|State|TaxReturnsFiled|TotalWages|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "|        PARC PARQUE|     US|        false|               null|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE| -66.22|         null|           1|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|PASEO COSTA DEL SUR|     US|        false|               null|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE| -66.22|         null|           2|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|  CINGULAR WIRELESS|     US|        false|               null|32.72|NA-US-TX-CINGULAR...|Cingular Wireless...|NOT ACCEPTABLE| -97.31|         null|       61391|   TX|           null|      null|         NA| -0.1|-0.83| 0.54|     UNIQUE|  76166|\n",
      "|         FORT WORTH|     US|        false|               4053|32.75| NA-US-TX-FORT WORTH|      Fort Worth, TX|       PRIMARY| -97.33|         null|       61392|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|           FT WORTH|     US|        false|               4053|32.75|   NA-US-TX-FT WORTH|        Ft Worth, TX|    ACCEPTABLE| -97.33|         null|       61393|   TX|           2126| 122396986|         NA| -0.1|-0.83| 0.54|   STANDARD|  76177|\n",
      "|    URB EUGENE RICE|     US|        false|               null|17.96|NA-US-PR-URB EUGE...| Urb Eugene Rice, PR|NOT ACCEPTABLE| -66.22|         null|           4|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|               MESA|     US|        false|              26883|33.37|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.64|no NWS data, |       39827|   AZ|          14962| 563792730|         NA| -0.3|-0.77| 0.55|   STANDARD|  85209|\n",
      "|               MESA|     US|        false|              25446|33.38|       NA-US-AZ-MESA|            Mesa, AZ|       PRIMARY|-111.84|         null|       39828|   AZ|          14374| 471000465|         NA|-0.31|-0.77| 0.55|   STANDARD|  85210|\n",
      "|           HILLIARD|     US|        false|               7443|30.69|   NA-US-FL-HILLIARD|        Hilliard, FL|       PRIMARY| -81.92|         null|       49345|   FL|           3922| 133112149|         NA| 0.12|-0.85| 0.51|   STANDARD|  32046|\n",
      "|             HOLDER|     US|        false|               null|28.96|     NA-US-FL-HOLDER|          Holder, FL|       PRIMARY| -82.41|         null|       49346|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34445|\n",
      "|               HOLT|     US|        false|               2190|30.72|       NA-US-FL-HOLT|            Holt, FL|       PRIMARY| -86.67|         null|       49347|   FL|           1207|  36395913|         NA| 0.04|-0.85| 0.51|   STANDARD|  32564|\n",
      "|          HOMOSASSA|     US|        false|               null|28.78|  NA-US-FL-HOMOSASSA|       Homosassa, FL|       PRIMARY| -82.61|         null|       49348|   FL|           null|      null|         NA| 0.11|-0.86| 0.48|     PO BOX|  34487|\n",
      "|       BDA SAN LUIS|     US|        false|               null|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE| -66.26|         null|          10|   PR|           null|      null|         NA| 0.38|-0.86| 0.31|   STANDARD|    708|\n",
      "|      SECT LANAUSSE|     US|        false|               null|17.96|NA-US-PR-SECT LAN...|   Sect Lanausse, PR|NOT ACCEPTABLE| -66.22|         null|           3|   PR|           null|      null|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|      SPRING GARDEN|     US|        false|               null|33.97|NA-US-AL-SPRING G...|   Spring Garden, AL|       PRIMARY| -85.55|         null|       54354|   AL|           null|      null|         NA| 0.06|-0.82| 0.55|     PO BOX|  36275|\n",
      "|        SPRINGVILLE|     US|        false|               7845|33.77|NA-US-AL-SPRINGVILLE|     Springville, AL|       PRIMARY| -86.47|         null|       54355|   AL|           4046| 172127599|         NA| 0.05|-0.82| 0.55|   STANDARD|  35146|\n",
      "|        SPRUCE PINE|     US|        false|               1209|34.37|NA-US-AL-SPRUCE PINE|     Spruce Pine, AL|       PRIMARY| -87.69|         null|       54356|   AL|            610|  18525517|         NA| 0.03|-0.82| 0.56|   STANDARD|  35585|\n",
      "|           ASH HILL|     US|        false|               1666| 36.4|   NA-US-NC-ASH HILL|        Ash Hill, NC|NOT ACCEPTABLE| -80.56|         null|       76511|   NC|            842|  28876493|         NA| 0.13|-0.79| 0.59|   STANDARD|  27007|\n",
      "|           ASHEBORO|     US|        false|              15228|35.71|   NA-US-NC-ASHEBORO|        Asheboro, NC|       PRIMARY| -79.81|         null|       76512|   NC|           8355| 215474318|         NA| 0.14|-0.79| 0.58|   STANDARD|  27203|\n",
      "+-------------------+-------+-------------+-------------------+-----+--------------------+--------------------+--------------+-------+-------------+------------+-----+---------------+----------+-----------+-----+-----+-----+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"zipcode.json\")\n",
    "df.printSchema()\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-----+-----------+-------+\n",
      "|               City|RecordNumber|State|ZipCodeType|Zipcode|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|           2|   PR|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|          10|   PR|   STANDARD|    709|\n",
      "+-------------------+------------+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multi-line JSON file into dataframe:\n",
    "\n",
    "multiline_df = spark.read.option(\"multiline\",\"true\") \\\n",
    "      .json(\"multiline-zipcode.json\")\n",
    "multiline_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|               City|Country|Decommisioned|  Lat|            Location|        LocationText|  LocationType|  Long|RecordNumber|State|WorldRegion|Xaxis|Yaxis|Zaxis|ZipCodeType|Zipcode|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "|PASEO COSTA DEL SUR|     US|        false|17.96|NA-US-PR-PASEO CO...|Paseo Costa Del S...|NOT ACCEPTABLE|-66.22|           2|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "|       BDA SAN LUIS|     US|        false|18.14|NA-US-PR-BDA SAN ...|    Bda San Luis, PR|NOT ACCEPTABLE|-66.26|          10|   PR|         NA| 0.38|-0.86| 0.31|   STANDARD|    709|\n",
      "|        PARC PARQUE|     US|        false|17.96|NA-US-PR-PARC PARQUE|     Parc Parque, PR|NOT ACCEPTABLE|-66.22|           1|   PR|         NA| 0.38|-0.87|  0.3|   STANDARD|    704|\n",
      "+-------------------+-------+-------------+-----+--------------------+--------------------+--------------+------+------------+-----+-----------+-----+-----+-----+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read multiple JSON files into dataframe:\n",
    "\n",
    "df2 = spark.read.json(\n",
    "    ['zipcode2.json','zipcode1.json'])\n",
    "df2.show(4)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all JSON files from a directory:\n",
    "\n",
    "\n",
    "# df3 = spark.read.json(\"*.json\")\n",
    "# df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m StructType,StructField, StringType, IntegerType,BooleanType,DoubleType\n\u001b[0;32m      5\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[0;32m      6\u001b[0m       StructField(\u001b[39m\"\u001b[39m\u001b[39mRecordNumber\u001b[39m\u001b[39m\"\u001b[39m,IntegerType(),\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m      7\u001b[0m       StructField(\u001b[39m\"\u001b[39m\u001b[39mZipcode\u001b[39m\u001b[39m\"\u001b[39m,IntegerType(),\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m       StructField(\u001b[39m\"\u001b[39m\u001b[39mNotes\u001b[39m\u001b[39m\"\u001b[39m,StringType(),\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m   ])\n\u001b[1;32m---> 28\u001b[0m df_with_schema \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mschema(schema)\u001b[39m.\u001b[39mjson(\u001b[39m\"\u001b[39m\u001b[39mzipcode.json\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m df_with_schema\u001b[39m.\u001b[39mprintSchema()\n\u001b[0;32m     30\u001b[0m df_with_schema\u001b[39m.\u001b[39mshow(\u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Define custom schema for zipcode.json, because zipcode.json files have not any schema information:\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,BooleanType,DoubleType\n",
    "schema = StructType([\n",
    "      StructField(\"RecordNumber\",IntegerType(),True),\n",
    "      StructField(\"Zipcode\",IntegerType(),True),\n",
    "      StructField(\"ZipCodeType\",StringType(),True),\n",
    "      StructField(\"City\",StringType(),True),\n",
    "      StructField(\"State\",StringType(),True),\n",
    "      StructField(\"LocationType\",StringType(),True),\n",
    "      StructField(\"Lat\",DoubleType(),True),\n",
    "      StructField(\"Long\",DoubleType(),True),\n",
    "      StructField(\"Xaxis\",IntegerType(),True),\n",
    "      StructField(\"Yaxis\",DoubleType(),True),\n",
    "      StructField(\"Zaxis\",DoubleType(),True),\n",
    "      StructField(\"WorldRegion\",StringType(),True),\n",
    "      StructField(\"Country\",StringType(),True),\n",
    "      StructField(\"LocationText\",StringType(),True),\n",
    "      StructField(\"Location\",StringType(),True),\n",
    "      StructField(\"Decommisioned\",BooleanType(),True),\n",
    "      StructField(\"TaxReturnsFiled\",StringType(),True),\n",
    "      StructField(\"EstimatedPopulation\",IntegerType(),True),\n",
    "      StructField(\"TotalWages\",IntegerType(),True),\n",
    "      StructField(\"Notes\",StringType(),True)\n",
    "  ])\n",
    "\n",
    "df_with_schema = spark.read.schema(schema).json(\"zipcode.json\")\n",
    "df_with_schema.printSchema()\n",
    "df_with_schema.show(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
